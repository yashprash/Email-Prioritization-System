{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email Prioritization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEGiqWhSHZ_U",
        "outputId": "462244dc-f28c-42e6-f2a4-fc0dc78b114d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyZAz_nWfTRk",
        "outputId": "0db4769b-57b1-4bf1-cf18-54d345477c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPqrjjC8BNwV",
        "outputId": "c7372831-3601-48ad-cc8b-1f063ea0f708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import json\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tag_dict={\n",
        "    'J':wordnet.ADJ,\n",
        "    'V':wordnet.VERB,\n",
        "    \"N\": wordnet.NOUN,\n",
        "    \"R\": wordnet.ADV\n",
        "}\n",
        "def get_pos_tag(word):\n",
        "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "def clean_text(text):\n",
        "    ## Remove puncuation\n",
        "    #text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^a-z0-9']\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" 's \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"0\",\" zero \", text)\n",
        "    text = re.sub(r\"1\",\" one \", text)\n",
        "    text = re.sub(r\"2\",\" two \", text)\n",
        "    text = re.sub(r\"3\",\" three \", text)\n",
        "    text = re.sub(r\"4\",\" four \", text)\n",
        "    text = re.sub(r\"5\",\" five \", text)\n",
        "    text = re.sub(r\"6\",\" six \", text)\n",
        "    text = re.sub(r\"7\",\" seven \", text)\n",
        "    text = re.sub(r\"8\",\" eight \", text)\n",
        "    text = re.sub(r\"9\",\" nine \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"\\n\",\"\",text)\n",
        "\n",
        "    sentence_clean=[]\n",
        "    for word in text.split():\n",
        "        sentence_clean.append(lemmatizer.lemmatize(word,get_pos_tag(word)))\n",
        "    return \" \".join(sentence_clean)\n",
        "\n",
        "\n",
        "l=[]\n",
        "count=0\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "fg=open(\"drive/My Drive/AIProject/dataset_values.txt\",\"w\")\n",
        "with open(\"drive/My Drive/AIProject/dataset.txt\") as f:\n",
        "    for line in f:\n",
        "        if count==20000:\n",
        "            break\n",
        "        elif re.match(\"^!!!!!\",line):\n",
        "            try:\n",
        "                s=\" \".join(l)\n",
        "                fin=clean_text(s)\n",
        "                fg.write(fin+\"\\n!!!!!\\n\")\n",
        "                count=count+1\n",
        "            except:\n",
        "                print(\"Oops!\",sys.exc_info()[0],\"occured.\")\n",
        "                pass\n",
        "            l=[]\n",
        "        else: \n",
        "            l.append(line)\n",
        "print(count)\n",
        "fg.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEn_8sv4BVMC"
      },
      "source": [
        "import json\n",
        "import re\n",
        "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
        "from watson_developer_cloud.natural_language_understanding_v1 import Features, CategoriesOptions\n",
        "\n",
        "\n",
        "\n",
        "def pre_process(email):\n",
        "    stopwords = [\"a\", \"share\", \"linkthese\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\", \"this\"];\n",
        "    text=list(email)\n",
        "    #print(text)\n",
        "    for i in text:\n",
        "        if i in (',','.','?','!'):\n",
        "            del text[text.index(i)]\n",
        "    email1=\"\".join(text)\n",
        "    text=email1.split(\" \")\n",
        "    final=[]\n",
        "    for word in text:\n",
        "        text[text.index(word)]=word.lower()\n",
        "    for word in text:\n",
        "        if word not in stopwords:\n",
        "            final.append(word)\n",
        "    return \" \".join(final)\n",
        "\n",
        "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
        "    version='2018-11-16',\n",
        "    iam_apikey='u8rI-gjW7N96229thht83BC2vl1SYJ0Xi1FCEih5dbGe',\n",
        "    url='https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
        ")\n",
        "l=[]\n",
        "count=0\n",
        "fh=open(\"C:\\AIProject\\dataset_label.txt\",\"w\")\n",
        "fg=open(\"C:\\AIProject\\dataset_values.txt\",\"w\")\n",
        "with open(\"C:\\AIProject\\dataset.txt\") as f:\n",
        "    for line in f:\n",
        "        if count==20000:\n",
        "            break\n",
        "        elif re.match(\"^!!!!\",line):\n",
        "            try:\n",
        "                s=\" \".join(l)\n",
        "                fin=pre_process(s)\n",
        "                response = natural_language_understanding.analyze(\n",
        "                text=fin,\n",
        "                features=Features(categories=CategoriesOptions(limit=1))).get_result()\n",
        "                fh.write(str(response[\"categories\"][0][\"label\"])+\"-\"+str(response[\"categories\"][0][\"score\"])+\"\\n\")\n",
        "                fg.write(fin+\"\\n\\n\")\n",
        "                count=count+1\n",
        "            except: \n",
        "                pass\n",
        "            l=[]\n",
        "        else: \n",
        "            l.append(line)\n",
        "print(count)\n",
        "fh.close()\n",
        "fg.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6507pJvDnW30",
        "outputId": "f1fb0dcf-0be4-47e5-bf09-aaccd4544338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1441
        }
      },
      "source": [
        "import numpy\n",
        "import scipy\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import json\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "  \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "embedding_matrix={}\n",
        "X=numpy.empty((20000,1),dtype=object)\n",
        "label=numpy.empty((20000,6))\n",
        "categories=[\"health and fitness\",\"business and real estate\",\"finance and politics\",\"science and technology\",\"sports and automotive\",\"art, entertainment and society\"]\n",
        "\n",
        "def load():\n",
        "  print(\"Loading X and label\")\n",
        "  global embedding_matrix,X,label,categories\n",
        "  count=0\n",
        "  with open(\"drive/My Drive/AIProject/dataset_values.txt\",\"r\") as f:\n",
        "    l=[]\n",
        "    for line in f:\n",
        "      if count>=20000:\n",
        "        break\n",
        "      if re.match(\"^!!!!!\",line):\n",
        "        s=\" \".join(l)\n",
        "        text=s.split()\n",
        "        for i in text:\n",
        "          text[text.index(i)]=text[text.index(i)].strip()\n",
        "        fin=[]\n",
        "        for word in text:\n",
        "          if word in embedding_matrix.keys():\n",
        "            #print(word)\n",
        "            fin.append(word)\n",
        "        l=[]\n",
        "        X[count][0]=str(\" \".join(fin))\n",
        "        count+=1\n",
        "      else:\n",
        "        l.append(line)\n",
        "  count=0\n",
        "  with open(\"drive/My Drive/AIProject/dataset_labels.txt\",\"r\") as f:\n",
        "    vector=[]\n",
        "    for i in range(6):\n",
        "      vector.append(0)\n",
        "    for line in f:\n",
        "      if count>=20000:\n",
        "        break\n",
        "      h=line.strip()\n",
        "      y=copy.deepcopy(vector)\n",
        "      y[categories.index(h)]=1\n",
        "      label[count]=y\n",
        "      count+=1\n",
        "  print(X.shape)\n",
        "  print(label.shape)\n",
        "  print(\"Done\")    \n",
        "\n",
        "def prepare_matrix():\n",
        "    global embedding_matrix\n",
        "    print(\"Preparing embedding matrix\")\n",
        "    fh=open(\"drive/My Drive/AIProject/embedding_matrix.pkl\",\"rb\")\n",
        "    embedding_matrix=pickle.load(fh)\n",
        "    fh.close()\n",
        "    print(len(embedding_matrix.keys()))\n",
        "    print(\"Done\")\n",
        "    \n",
        "def prepare_for_model():\n",
        "  print(\"Preparing for model\")\n",
        "  global X,label,embedding_matrix\n",
        "  tokenizer = Tokenizer(num_words= 400000)\n",
        "  tokenizer.fit_on_texts(numpy.asarray(list(embedding_matrix.keys())))\n",
        "  fg=open(\"drive/My Drive/AIProject/embeddings.pkl\",\"rb\")\n",
        "  embeddings=pickle.load(fg)\n",
        "  fg.close()\n",
        "  sequences = tokenizer.texts_to_sequences(X[:,0])\n",
        "  data = pad_sequences(sequences,maxlen=30)\n",
        "  print(\"Done\")\n",
        "  return data,embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def train_using_API(data,embeddings):\n",
        "  global label\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=400000,output_dim=300,weights=[embeddings],trainable=False,mask_zero=True))\n",
        "  model.add(Masking(mask_value=0.0))\n",
        "  model.add(LSTM(64, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "  model.add(Dense(64, activation='tanh'))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  #x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.25, shuffle= True)\n",
        "  model.fit(data, label, batch_size=32, validation_split=0.25, epochs=100)\n",
        "  #scores = model.evaluate(x_test, y_test)\n",
        "  #print(\"Accuracy:\", scores[1])  \n",
        "  ff=open(\"drive/My Drive/AIProject/model_6.pkl\",\"wb\")\n",
        "  pickle.dump(model,ff)\n",
        "  ff.close()\n",
        "      \n",
        "\n",
        "prepare_matrix()\n",
        "load()\n",
        "d,e=prepare_for_model()\n",
        "train_using_API(d,e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "400000\n",
            "Done\n",
            "Loading X and label\n",
            "(20000, 1)\n",
            "(20000, 6)\n",
            "Done\n",
            "Preparing for model\n",
            "Done\n",
            "Train on 15000 samples, validate on 5000 samples\n",
            "Epoch 1/100\n",
            "15000/15000 [==============================] - 28s 2ms/step - loss: 1.7896 - acc: 0.2031 - val_loss: 1.7892 - val_acc: 0.1970\n",
            "Epoch 2/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.7608 - acc: 0.2305 - val_loss: 1.7957 - val_acc: 0.1894\n",
            "Epoch 3/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.7505 - acc: 0.2384 - val_loss: 1.7877 - val_acc: 0.2090\n",
            "Epoch 4/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.7310 - acc: 0.2529 - val_loss: 1.8004 - val_acc: 0.2022\n",
            "Epoch 5/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.7124 - acc: 0.2730 - val_loss: 1.8150 - val_acc: 0.1902\n",
            "Epoch 6/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.6940 - acc: 0.2805 - val_loss: 1.8365 - val_acc: 0.1834\n",
            "Epoch 7/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.6725 - acc: 0.2980 - val_loss: 1.8372 - val_acc: 0.1998\n",
            "Epoch 8/100\n",
            "15000/15000 [==============================] - 26s 2ms/step - loss: 1.6506 - acc: 0.3084 - val_loss: 1.8496 - val_acc: 0.1942\n",
            "Epoch 9/100\n",
            "15000/15000 [==============================] - 27s 2ms/step - loss: 1.6276 - acc: 0.3229 - val_loss: 1.8726 - val_acc: 0.2052\n",
            "Epoch 10/100\n",
            "14752/15000 [============================>.] - ETA: 0s - loss: 1.6029 - acc: 0.3330"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-388729fcc1c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mtrain_using_API\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-388729fcc1c8>\u001b[0m in \u001b[0;36mtrain_using_API\u001b[0;34m(data, embeddings)\u001b[0m\n\u001b[1;32m    109\u001b[0m     optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\u001b[1;32m    110\u001b[0m   \u001b[0;31m#x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.25, shuffle= True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0;31m#scores = model.evaluate(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;31m#print(\"Accuracy:\", scores[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry2R1HeWsVy8",
        "outputId": "631ecce2-fe1c-4183-831a-7d16b367e3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(label[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzrd-Fnptf-9",
        "outputId": "56bed7d7-273e-4c06-e9a3-438d2281680e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2060
        }
      },
      "source": [
        "import numpy\n",
        "import scipy\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import json\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "  \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "embedding_matrix={}\n",
        "X=numpy.empty((9993,1),dtype=object)\n",
        "label=numpy.empty((9993,6))\n",
        "categories=[\"business and industrial\",\"science and technology\",\"finance and news\",\"education and government\",\"fitness and automotive\",\"art, entertainment and society\"]\n",
        "\n",
        "def load():\n",
        "  print(\"Loading X and label\")\n",
        "  global embedding_matrix,X,label,categories\n",
        "  count=0\n",
        "  with open(\"drive/My Drive/AIProject/category.txt\",\"r\") as f:\n",
        "    l=[]\n",
        "    for line in f:\n",
        "      if re.match(\"^/\",line):\n",
        "        s=\" \".join(l)\n",
        "        text=s.split()\n",
        "        for i in text:\n",
        "          text[text.index(i)]=text[text.index(i)].strip()\n",
        "        fin=[]\n",
        "        for word in text:\n",
        "          if word in embedding_matrix.keys():\n",
        "            #print(word)\n",
        "            fin.append(word)\n",
        "        l=[]\n",
        "        X[count][0]=str(\" \".join(fin))\n",
        "        vector=[]\n",
        "        for i in range(6):\n",
        "          vector.append(0)\n",
        "        h=line.split(\"/\")\n",
        "        if \"-\" in h[1]:\n",
        "          j=h[1].split(\"-\")\n",
        "          h[1]=j[0]\n",
        "        if h[1] in (\"business and industrial\"):\n",
        "          cat=\"business and industrial\"\n",
        "        elif h[1] in (\"technology and computing\",\"science\"):\n",
        "          cat=\"science and technology\"\n",
        "        elif h[1] in (\"finance\",\"news\",\"real estate\"):\n",
        "          cat=\"finance and news\"\n",
        "        elif h[1] in (\"law, govt and politics\",\"careers\",\"education\"):\n",
        "          cat=\"education and government\"\n",
        "        elif h[1] in (\"sports\",\"automotive and vehicles\",\"health and fitness\"):\n",
        "          cat=\"fitness and automotive\"\n",
        "        else:\n",
        "          cat=\"art, entertainment and society\"\n",
        "        y=copy.deepcopy(vector)\n",
        "        y[categories.index(cat)]=1\n",
        "        label[count]=y\n",
        "        count+=1\n",
        "      else:\n",
        "        l.append(line)\n",
        "  print(X.shape)\n",
        "  print(label.shape)\n",
        "  print(\"Done\")    \n",
        "\n",
        "def prepare_matrix():\n",
        "    global embedding_matrix\n",
        "    print(\"Preparing embedding matrix\")\n",
        "    fh=open(\"drive/My Drive/AIProject/embedding_matrix.pkl\",\"rb\")\n",
        "    embedding_matrix=pickle.load(fh)\n",
        "    fh.close()\n",
        "    print(len(embedding_matrix.keys()))\n",
        "    print(\"Done\")\n",
        "    \n",
        "def prepare_for_model():\n",
        "  print(\"Preparing for model\")\n",
        "  global X,label,embedding_matrix\n",
        "  tokenizer = Tokenizer(num_words= 400000)\n",
        "  tokenizer.fit_on_texts(numpy.asarray(list(embedding_matrix.keys())))\n",
        "  fg=open(\"drive/My Drive/AIProject/embeddings.pkl\",\"rb\")\n",
        "  embeddings=pickle.load(fg)\n",
        "  fg.close()\n",
        "  sequences = tokenizer.texts_to_sequences(X[:,0])\n",
        "  data = pad_sequences(sequences,maxlen=50)\n",
        "  print(\"Done\")\n",
        "  return data,embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def train(data,embeddings):\n",
        "  global label\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=400000,output_dim=300,weights=[embeddings],trainable=False,mask_zero=True))\n",
        "  model.add(Masking(mask_value=0.0))\n",
        "  model.add(LSTM(64, return_sequences=False, \n",
        "               dropout=0.5, recurrent_dropout=0.5))\n",
        "\n",
        "  #model.add(Dense(64, activation='relu'))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  #x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.25, shuffle= True)\n",
        "  model.fit(data, label, batch_size=32, validation_split=0.3, epochs=50)\n",
        "  ff=open(\"drive/My Drive/AIProject/model_6.pkl\",\"wb\")\n",
        "  pickle.dump(model,ff)\n",
        "  ff.close()\n",
        "      \n",
        "\n",
        "prepare_matrix()\n",
        "load()\n",
        "d,e=prepare_for_model()\n",
        "train(d,e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "400000\n",
            "Done\n",
            "Loading X and label\n",
            "(9993, 1)\n",
            "(9993, 6)\n",
            "Done\n",
            "Preparing for model\n",
            "Done\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6995 samples, validate on 2998 samples\n",
            "Epoch 1/50\n",
            "6995/6995 [==============================] - 21s 3ms/step - loss: 1.5608 - acc: 0.3751 - val_loss: 1.3772 - val_acc: 0.4206\n",
            "Epoch 2/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.3520 - acc: 0.4905 - val_loss: 1.2804 - val_acc: 0.5137\n",
            "Epoch 3/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.2603 - acc: 0.5337 - val_loss: 1.2228 - val_acc: 0.5440\n",
            "Epoch 4/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.1917 - acc: 0.5675 - val_loss: 1.2143 - val_acc: 0.5600\n",
            "Epoch 5/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.1507 - acc: 0.5811 - val_loss: 1.1963 - val_acc: 0.5570\n",
            "Epoch 6/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.1190 - acc: 0.6006 - val_loss: 1.1691 - val_acc: 0.5714\n",
            "Epoch 7/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.0816 - acc: 0.6166 - val_loss: 1.1612 - val_acc: 0.5740\n",
            "Epoch 8/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.0461 - acc: 0.6340 - val_loss: 1.1808 - val_acc: 0.5727\n",
            "Epoch 9/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.0283 - acc: 0.6322 - val_loss: 1.1439 - val_acc: 0.5811\n",
            "Epoch 10/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 1.0056 - acc: 0.6450 - val_loss: 1.1589 - val_acc: 0.5730\n",
            "Epoch 11/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9865 - acc: 0.6515 - val_loss: 1.1403 - val_acc: 0.5807\n",
            "Epoch 12/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9610 - acc: 0.6659 - val_loss: 1.1397 - val_acc: 0.5737\n",
            "Epoch 13/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9549 - acc: 0.6648 - val_loss: 1.1404 - val_acc: 0.5801\n",
            "Epoch 14/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9371 - acc: 0.6688 - val_loss: 1.1508 - val_acc: 0.5857\n",
            "Epoch 15/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9199 - acc: 0.6711 - val_loss: 1.1509 - val_acc: 0.5907\n",
            "Epoch 16/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.9098 - acc: 0.6786 - val_loss: 1.1647 - val_acc: 0.5817\n",
            "Epoch 17/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8983 - acc: 0.6821 - val_loss: 1.1502 - val_acc: 0.5871\n",
            "Epoch 18/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8872 - acc: 0.6869 - val_loss: 1.1442 - val_acc: 0.5864\n",
            "Epoch 19/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8794 - acc: 0.6904 - val_loss: 1.1390 - val_acc: 0.5871\n",
            "Epoch 20/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8538 - acc: 0.6955 - val_loss: 1.1666 - val_acc: 0.5781\n",
            "Epoch 21/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8407 - acc: 0.7031 - val_loss: 1.1664 - val_acc: 0.5844\n",
            "Epoch 22/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8410 - acc: 0.6955 - val_loss: 1.1452 - val_acc: 0.5901\n",
            "Epoch 23/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8292 - acc: 0.7092 - val_loss: 1.1659 - val_acc: 0.5824\n",
            "Epoch 24/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8161 - acc: 0.7145 - val_loss: 1.1632 - val_acc: 0.5867\n",
            "Epoch 25/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8072 - acc: 0.7175 - val_loss: 1.1823 - val_acc: 0.5767\n",
            "Epoch 26/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.8023 - acc: 0.7142 - val_loss: 1.1763 - val_acc: 0.5917\n",
            "Epoch 27/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7886 - acc: 0.7225 - val_loss: 1.1735 - val_acc: 0.5881\n",
            "Epoch 28/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7888 - acc: 0.7239 - val_loss: 1.1917 - val_acc: 0.5754\n",
            "Epoch 29/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7887 - acc: 0.7235 - val_loss: 1.1731 - val_acc: 0.5954\n",
            "Epoch 30/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7831 - acc: 0.7287 - val_loss: 1.1756 - val_acc: 0.5884\n",
            "Epoch 31/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7734 - acc: 0.7320 - val_loss: 1.1713 - val_acc: 0.5911\n",
            "Epoch 32/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7668 - acc: 0.7277 - val_loss: 1.1760 - val_acc: 0.5871\n",
            "Epoch 33/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7585 - acc: 0.7272 - val_loss: 1.1850 - val_acc: 0.5907\n",
            "Epoch 34/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7476 - acc: 0.7382 - val_loss: 1.2090 - val_acc: 0.5881\n",
            "Epoch 35/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7474 - acc: 0.7377 - val_loss: 1.2012 - val_acc: 0.5884\n",
            "Epoch 36/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7542 - acc: 0.7315 - val_loss: 1.1900 - val_acc: 0.6021\n",
            "Epoch 37/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7408 - acc: 0.7380 - val_loss: 1.1907 - val_acc: 0.5907\n",
            "Epoch 38/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7447 - acc: 0.7412 - val_loss: 1.1917 - val_acc: 0.5874\n",
            "Epoch 39/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7324 - acc: 0.7441 - val_loss: 1.2222 - val_acc: 0.5864\n",
            "Epoch 40/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7359 - acc: 0.7421 - val_loss: 1.2138 - val_acc: 0.5837\n",
            "Epoch 41/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7226 - acc: 0.7500 - val_loss: 1.1935 - val_acc: 0.5914\n",
            "Epoch 42/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7217 - acc: 0.7471 - val_loss: 1.2157 - val_acc: 0.5907\n",
            "Epoch 43/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7204 - acc: 0.7484 - val_loss: 1.2184 - val_acc: 0.5884\n",
            "Epoch 44/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7244 - acc: 0.7501 - val_loss: 1.1938 - val_acc: 0.5874\n",
            "Epoch 45/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7097 - acc: 0.7465 - val_loss: 1.2160 - val_acc: 0.5874\n",
            "Epoch 46/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7062 - acc: 0.7545 - val_loss: 1.2067 - val_acc: 0.5911\n",
            "Epoch 47/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.6973 - acc: 0.7495 - val_loss: 1.2229 - val_acc: 0.5891\n",
            "Epoch 48/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7038 - acc: 0.7555 - val_loss: 1.2063 - val_acc: 0.5927\n",
            "Epoch 49/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.7066 - acc: 0.7510 - val_loss: 1.2178 - val_acc: 0.5931\n",
            "Epoch 50/50\n",
            "6995/6995 [==============================] - 19s 3ms/step - loss: 0.6821 - acc: 0.7683 - val_loss: 1.2370 - val_acc: 0.5917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj-jTPSnwxOX"
      },
      "source": [
        "global label\n",
        "num=[]\n",
        "for i in range(6):\n",
        "  num.append(0)\n",
        "for vec in label:\n",
        "  for i in range(6):\n",
        "    if vec[i]==1:\n",
        "      x=i\n",
        "      break\n",
        "  num[x]+=1\n",
        "print(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU0tLzAUgoKx",
        "outputId": "bea73b0e-1348-4c7e-f657-76beac515996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import numpy\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "  \n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "categories=[\"business and industrial\",\"science and technology\",\"finance and news\",\"education and government\",\"fitness and automotive\",\"art, entertainment and society\"]\n",
        "\n",
        "embedding_matrix={}\n",
        "tag_dict={\n",
        "    'J':wordnet.ADJ,\n",
        "    'V':wordnet.VERB,\n",
        "    \"N\": wordnet.NOUN,\n",
        "    \"R\": wordnet.ADV\n",
        "}\n",
        "def get_pos_tag(word):\n",
        "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "def clean_text(text):\n",
        "    \n",
        "    text = text.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]   \n",
        "    text = \" \".join(text)    \n",
        "    text = re.sub(r\"[^a-z0-9']\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" 's \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"0\",\" zero \", text)\n",
        "    text = re.sub(r\"1\",\" one \", text)\n",
        "    text = re.sub(r\"2\",\" two \", text)\n",
        "    text = re.sub(r\"3\",\" three \", text)\n",
        "    text = re.sub(r\"4\",\" four \", text)\n",
        "    text = re.sub(r\"5\",\" five \", text)\n",
        "    text = re.sub(r\"6\",\" six \", text)\n",
        "    text = re.sub(r\"7\",\" seven \", text)\n",
        "    text = re.sub(r\"8\",\" eight \", text)\n",
        "    text = re.sub(r\"9\",\" nine \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"\\n\",\"\",text)\n",
        "\n",
        "    sentence_clean=[]\n",
        "    for word in text.split():\n",
        "        sentence_clean.append(lemmatizer.lemmatize(word,get_pos_tag(word)))\n",
        "    return \" \".join(sentence_clean)\n",
        "\n",
        "  \n",
        "def prepare_for_model(text):\n",
        "  global embedding_matrix\n",
        "  print(\"Preparing for model\")\n",
        "  fh=open(\"drive/My Drive/AIProject/embeddings.pkl\",\"rb\")\n",
        "  embeddings=pickle.load(fh)\n",
        "  fh.close()\n",
        "  ff=open(\"drive/My Drive/AIProject/embedding_matrix.pkl\",\"rb\")\n",
        "  embedding_matrix=pickle.load(ff)\n",
        "  ff.close()\n",
        "  tokenizer = Tokenizer(num_words= 400000)\n",
        "  tokenizer.fit_on_texts(numpy.asarray(list(embedding_matrix.keys())))\n",
        "  sequences = tokenizer.texts_to_sequences([text])\n",
        "  data = pad_sequences(sequences,maxlen=50)\n",
        "  print(\"Done\")\n",
        "  return data,embeddings\n",
        "\n",
        "def model(test,embeddings):\n",
        "  global categories\n",
        "  fg=open(\"drive/My Drive/AIProject/model_6.pkl\",\"rb\")\n",
        "  model=pickle.load(fg)\n",
        "  fg.close()\n",
        "  out=model.predict(test)\n",
        "  print(\"Output vector is: \"+str(out))\n",
        "  r=numpy.argmax(out)\n",
        "  print(\"Category match: \"+categories[r])\n",
        "  \n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "test=\"\"\"Learning Takes You Everywhere. Explore knowledge and new skills with the best universities in the world. ... New Courses and Specializations Explore New Courses Learn high-impact career skills and earn a course certificate.Predictive ew! Python Data Products for Predictive Analytics UC San Diego Data products and predictive analytics are powering the AI revolution. Use Python and machine learning to start collecting, modeling, and deploying data-driven systems today.\n",
        "Use Python to create interactive data visualizations and build demo systems\n",
        "Transform real-world data for data-driven predictive tasks\n",
        "Perform simple regressions and classifications using machine learning libraries\n",
        "Discover predictive analytics and take your Python skills to the next level with this four-course Specialization from UC San Diego.\"\"\"\n",
        "test=clean_text(test)\n",
        "d,e=prepare_for_model(test)\n",
        "model(d,e)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Preparing for model\n",
            "Done\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Output vector is: [[1.7045073e-02 9.7869587e-01 2.3966162e-03 3.2284905e-04 4.0857447e-04\n",
            "  1.1310281e-03]]\n",
            "Category match: science and technology\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QvjgHsbigPH"
      },
      "source": [
        "categories=[\"art and entertainment\",\"automotive and vehicles\",\"business and industrial\",\"careers\",\"education\",\"family and parenting\",\"finance\",\"food and drink\",\"health and fitness\",\"hobbies and interests\",\"home and garden\",\"law, govt and politics\",\"news\",\"pets\",\"real estate\",\"religion and spirituality\",\"science\",\"shopping\",\"society\",\"sports\",\"style and fashion\",\"technology and computing\",\"travel\"]\n",
        "\n",
        "fh=open(\"drive/My Drive/AIProject/dataset_labels.txt\",\"w\")\n",
        "with open(\"drive/My Drive/AIProject/dataset_label.txt\",\"r\") as f:\n",
        "  for line in f:\n",
        "    cat=\"\"\n",
        "    h=line.split(\"/\")\n",
        "    if \"-\" in h[1]:\n",
        "      j=h[1].split(\"-\")\n",
        "      h[1]=j[0]\n",
        "    if h[1] in (\"health and fitness\"):\n",
        "      cat=\"health and fitness\"\n",
        "    elif h[1] in (\"business and industrial\",\"real estate\"):\n",
        "      cat=\"business and real estate\"\n",
        "    elif h[1] in (\"finance\",\"news\",\"careers\",\"law, govt and politics\"):\n",
        "      cat=\"finance and politics\"\n",
        "    elif h[1] in (\"technology and computing\",\"science\",\"education\"):\n",
        "      cat=\"science and technology\"\n",
        "    elif h[1] in (\"sports\",\"automotive and vehicles\"):\n",
        "      cat=\"sports and automotive\"\n",
        "    else:\n",
        "      cat=\"art, entertainment and society\"\n",
        "    fh.write(str(cat)+\"\\n\")\n",
        "    \n",
        "fh.close()\n",
        "   \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6zkD8kSQ7E",
        "outputId": "59ed64ad-e812-4a37-97e7-7c33950d1d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "a=numpy.array([[1],[2],[3],[4],[5],[6]])\n",
        "r=numpy.argmax(a)\n",
        "print(r)\n",
        "#print(r[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QTCfd17OAno"
      },
      "source": [
        "\"import numpy\n",
        "import scipy\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "  \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def prepare_for_model():\n",
        "  global X,label,embedding_matrix\n",
        "  tokenizer = Tokenizer(num_words= 400000)\n",
        "  tokenizer.fit_on_texts(numpy.asarray(list(embedding_matrix.keys())))\n",
        "  embeddings = numpy.zeros((400000, 300))\n",
        "  for word,number in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_matrix.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings[number]=embedding_vector\n",
        "  sequences = tokenizer.texts_to_sequences(X[:,0])\n",
        "  data = pad_sequences(sequences)\n",
        "  return data,embeddings\n",
        "\n",
        "def prepare_matrix():\n",
        "    global embedding_matrix\n",
        "    print(\"Preparing embedding matrix\")\n",
        "    with open(\"drive/My Drive/AIProject/glove.6B.300d.txt\",\"r\") as f:\n",
        "        for line in f:\n",
        "            row=line.split()\n",
        "            word=row[0]\n",
        "            del row[0]\n",
        "            for i in range(300):\n",
        "                row[i]=float(row[i])\n",
        "            embedding_matrix[word]=list(row)\n",
        "    print(len(embedding_matrix.keys()))\n",
        "    print(\"Done\")\n",
        "\n",
        "  \n",
        "d,e=prepare_for_model()\n",
        "prepare_matrix()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reacSPUecd7n"
      },
      "source": [
        "import numpy\n",
        "import scipy\n",
        "import re\n",
        "import copy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "  \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "\n",
        "embedding_matrix={}\n",
        "X=[]\n",
        "label=[]\n",
        "categories=[\"art and entertainment\",\"automotive and vehicles\",\"business and industrial\",\"careers\",\"education\",\"family and parenting\",\"finance\",\"food and drink\",\"health and fitness\",\"hobbies and interests\",\"home and garden\",\"law, govt and politics\",\"news\",\"pets\",\"real estate\",\"religion and spirituality\",\"science\",\"shopping\",\"society\",\"sports\",\"style and fashion\",\"technology and computing\",\"travel\"]\n",
        "\n",
        "def load():\n",
        "  print(\"Loading X and label\")\n",
        "  global embedding_matrix,X,label,categories\n",
        "  with open(\"drive/My Drive/AIProject/dataset_values.txt\",\"r\") as f:\n",
        "    l=[]\n",
        "    for line in f:\n",
        "      if re.match(\"^!!!!!\",line):\n",
        "        s=\" \".join(l)\n",
        "        text=s.split()\n",
        "        for i in text:\n",
        "          text[text.index(i)]=text[text.index(i)].strip()\n",
        "        fin=[]\n",
        "        for word in text:\n",
        "          if word in embedding_matrix.keys():\n",
        "            #print(word)\n",
        "            fin.append(word)\n",
        "        l=[]\n",
        "        X.append(fin)\n",
        "      else:\n",
        "        l.append(line)\n",
        "  with open(\"drive/My Drive/AIProject/dataset_label.txt\",\"r\") as f:\n",
        "    vector=[]\n",
        "    for i in range(23):\n",
        "      vector.append(0)\n",
        "    for line in f:\n",
        "      h=line.split(\"/\")\n",
        "      if \"-\" in h[1]:\n",
        "        j=h[1].split(\"-\")\n",
        "        h[1]=j[0]\n",
        "      y=copy.deepcopy(vector)\n",
        "      y[categories.index(h[1])]=1\n",
        "      label.append(y)\n",
        "  print(\"Done\")    \n",
        "\n",
        "def prepare_matrix():\n",
        "    global embedding_matrix\n",
        "    print(\"Preparing embedding matrix\")\n",
        "    with open(\"drive/My Drive/AIProject/glove.6B.300d.txt\",\"r\") as f:\n",
        "        for line in f:\n",
        "            row=line.split()\n",
        "            word=row[0]\n",
        "            del row[0]\n",
        "            for i in range(300):\n",
        "                row[i]=float(row[i])\n",
        "            embedding_matrix[word]=list(row)\n",
        "    print(len(embedding_matrix.keys()))\n",
        "    print(\"Done\")\n",
        "\"\"\"    \n",
        "def pre_process(email):\n",
        "    stopwords = [\"a\", \"share\", \"linkthese\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\",\"\", \"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\", \"this\"];\n",
        "    text=list(email)\n",
        "    #print(text)\n",
        "    for i in text:\n",
        "        if i in (',','.','?'):\n",
        "            del text[text.index(i)]\n",
        "    email1=\"\".join(text)\n",
        "    text=email1.split(\" \")\n",
        "    final=[]\n",
        "    for word in text:\n",
        "        text[text.index(word)]=word.lower()\n",
        "    for word in text:\n",
        "        if word not in stopwords:\n",
        "            final.append(word)\n",
        "    return final\n",
        "\"\"\"\n",
        "    \n",
        "def sigmoidGrad(z):\n",
        "    r=z.shape[0]\n",
        "    c=z.shape[1]\n",
        "    res=numpy.zeros((r,c))\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            res[i][j]=z[i][j]*(1-z[i][j])\n",
        "    return res\n",
        "        \n",
        "\"\"\"   \n",
        "def forw_back_prop(n,email_vector,y):\n",
        "    global W,theta,loss,delta1,delta2\n",
        "\"\"\"\n",
        "def sigmoid(z):\n",
        "    r=z.shape[0]\n",
        "    c=z.shape[1]\n",
        "    y=numpy.zeros((r,c))\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            y[i][j]=1.0/(1.0+numpy.exp(-z[i][j]))\n",
        "    return y\n",
        "\n",
        "def comp_cost(nn_params):\n",
        "    global embedding_matrix,X,label,train_count\n",
        "    train_count=0\n",
        "    loss=0\n",
        "    W=numpy.zeros((1,301))\n",
        "    theta=numpy.zeros((23,1))\n",
        "    for i in range(301):\n",
        "        W[0][i]=nn_params[i]\n",
        "    for j in range(23):\n",
        "        theta[j][0]=nn_params[301+j]\n",
        "    for i in range(0,15000):\n",
        "        email_text=X[i]\n",
        "        #print(email_text)\n",
        "        if len(email_text)<2:\n",
        "          continue\n",
        "        train_count+=1\n",
        "        y=numpy.zeros((23,1))\n",
        "        for k in range(23):\n",
        "          y[k][0]=label[i][k]\n",
        "        a=numpy.zeros((len(email_text)+1,1,1))\n",
        "        a[0]=0\n",
        "        for i in range(1,len(email_text)+1):\n",
        "            ax=numpy.zeros((301,1))\n",
        "            ax[0]=a[i-1]\n",
        "            word_embed=embedding_matrix[email_text[i-1]]\n",
        "            for j in range(300):\n",
        "                ax[1+j][0]=word_embed[j]\n",
        "            a[i]=numpy.tanh(numpy.dot(W,ax))\n",
        "        output=sigmoid(numpy.dot(theta,a[len(email_text)]))\n",
        "        c=[]\n",
        "        csum=0\n",
        "        for i in range(23):\n",
        "            c.append((y[i]*numpy.log(output[i][0]))+((1-y[i])*numpy.log(1-output[i][0])))\n",
        "            csum=csum+c[i]\n",
        "        loss=loss+csum\n",
        "    print(train_count)\n",
        "    J=(-1/train_count)*loss\n",
        "    return J\n",
        "\n",
        "def comp_gradient(nn_params):\n",
        "    global embedding_matrix,X,label\n",
        "    train_count=0\n",
        "    delta3=numpy.zeros((23,1))\n",
        "    delta2=numpy.zeros((1,1))\n",
        "    delta1=numpy.zeros((1,1))\n",
        "    W=numpy.zeros((1,301))\n",
        "    theta=numpy.zeros((23,1))\n",
        "    for i in range(301):\n",
        "        W[0][i]=nn_params[i]\n",
        "    for j in range(23):\n",
        "        theta[j][0]=nn_params[301+j]\n",
        "    for i in range(0,15000):\n",
        "        email_text=X[i]\n",
        "        if len(email_text)<2:\n",
        "          continue\n",
        "        train_count+=1\n",
        "        y=numpy.zeros((23,1))\n",
        "        for k in range(23):\n",
        "          y[k][0]=label[i][k]\n",
        "        a=numpy.zeros((len(email_text)+1,1,1))\n",
        "        a[0]=0\n",
        "        for i in range(1,len(email_text)+1):\n",
        "            ax=numpy.zeros((301,1))\n",
        "            ax[0]=a[i-1]\n",
        "            word_embed=embedding_matrix[email_text[i-1]]\n",
        "            for j in range(300):\n",
        "                ax[1+j][0]=word_embed[j]\n",
        "            a[i]=numpy.tanh(numpy.dot(W,ax))\n",
        "        output=sigmoid(numpy.dot(theta,a[len(email_text)]))\n",
        "        del4=numpy.subtract(y,output)\n",
        "        del3=numpy.dot((numpy.dot(numpy.transpose(theta),del4)),(sigmoidGrad(a[len(email_text)])))\n",
        "        del2=numpy.dot((numpy.dot(numpy.transpose(W),del3)),(1-(numpy.tanh(a[len(email_text)-1]))**2))\n",
        "        delta1=numpy.add(delta1,numpy.dot(del2,numpy.transpose(a[len(email_text)-2])))\n",
        "        delta2=numpy.add(delta2,numpy.dot(del3,numpy.transpose(a[len(email_text)-1])))\n",
        "        delta3=numpy.add(delta3,numpy.dot(del4,numpy.transpose(a[len(email_text)])))\n",
        "    grad=[]\n",
        "    grad.append(delta1[0][0])\n",
        "    grad.append(delta2[0][0])\n",
        "    for i in range(23):\n",
        "        grad.append(delta3[i][0])\n",
        "    grad=numpy.ravel(grad)\n",
        "    return grad\n",
        "\n",
        "def train():\n",
        "    global X,label\n",
        "    print(\"Training\")\n",
        "    W=numpy.zeros((1,301))\n",
        "    for i in range(301):\n",
        "      W[0][i]=random.random()\n",
        "    theta=numpy.zeros((23,1))\n",
        "    for i in range(23):\n",
        "      theta[i][0]=random.random()\n",
        "    #Load email vectors to X and labels to label\n",
        "    para=[]\n",
        "    for i in range(301):\n",
        "        para.append(W[0][i])\n",
        "    for i in range(23):\n",
        "        para.append(theta[i][0])\n",
        "    initial_nn_params=numpy.ravel(para)\n",
        "    #costFunction=comp_cost(nn_params,X,label)\n",
        "    #nn_grad=comp_gradient(nn_params,X,label)\n",
        "    final_nn_params=scipy.optimize.fmin_cg(comp_cost,x0=initial_nn_params,maxiter=50)\n",
        "    print(final_nn_params.shape)\n",
        "    for i in range(301):\n",
        "        W[0][i]=final_nn_params[i]\n",
        "    for j in range(23):\n",
        "        theta[j][0]=final_nn_params[301+j]\n",
        "    return W,theta\n",
        "    print(\"Done\")\n",
        "    \n",
        "def train_using_API():\n",
        "  global X,label,embedding_matrix\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=1000,output_dim=300,weights=[embedding_matrix],trainable=False,mask_zero=True))\n",
        "  model.add(Masking(mask_value=0.0))\n",
        "  model.add(LSTM(64, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(X[:15000], label[:15000], batch_size=2048, epochs=10)\n",
        "  scores = model.evaluate(X[15000:], label[15000:])\n",
        "  print(\"Accuracy:\", scores[1])  \n",
        "    \n",
        "def test_avg(W,theta):\n",
        "  print(\"Testing\")\n",
        "  global embedding_matrix,X,label\n",
        "  err_sum=0\n",
        "  for i in range(15000,20000):\n",
        "    email_text=X[i]\n",
        "    y=numpy.zeros((23,1))\n",
        "    for k in range(23):\n",
        "      y[k][0]=label[i][k]\n",
        "    a=numpy.zeros((len(email_text)+1,1,1))\n",
        "    a[0]=0\n",
        "    for i in range(1,len(email_text)+1):\n",
        "      ax=numpy.zeros((301,1))\n",
        "      ax[0]=a[i-1]\n",
        "      word_embed=embedding_matrix[email_text[i-1]]\n",
        "      for j in range(300):\n",
        "        ax[1+j][0]=word_embed[j]\n",
        "      a[i]=numpy.tanh(numpy.dot(W,ax))\n",
        "    output=sigmoid(numpy.dot(theta,a[len(email_text)]))\n",
        "    err=0\n",
        "    for i in range(23):\n",
        "      err+=(y[i]-output[i])\n",
        "    err_sum+=(err/23.0)\n",
        "  return (1-(err_sum/5000))\n",
        "  print(\"Done\")\n",
        "  \n",
        "def test_classifier(W,theta):\n",
        "  print(\"Testing\")\n",
        "  global embedding_matrix,X,label\n",
        "  y_pred=[]\n",
        "  y_actual=[]\n",
        "  err_sum=0\n",
        "  for i in range(15000,20000):\n",
        "    email_text=X[i]\n",
        "    y_actual.append(label[i].index(1))\n",
        "    a=numpy.zeros((len(email_text)+1,1,1))\n",
        "    a[0]=0\n",
        "    for i in range(1,len(email_text)+1):\n",
        "      ax=numpy.zeros((301,1))\n",
        "      ax[0]=a[i-1]\n",
        "      word_embed=embedding_matrix[email_text[i-1]]\n",
        "      for j in range(300):\n",
        "        ax[1+j][0]=word_embed[j]\n",
        "      a[i]=numpy.tanh(numpy.dot(W,ax))\n",
        "    output=sigmoid(numpy.dot(theta,a[len(email_text)]))\n",
        "    y_pred.append(output.index(max(output)))\n",
        "  return sklearn.metrics.accuracy_score(y_actual,y_pred)\n",
        "  print(\"Done\")\n",
        "  \n",
        "prepare_matrix()\n",
        "load()\n",
        "#W_fin,theta_fin=train()\n",
        "#print(\"Accuracy (average) of model is \"+str(test_avg(W_fin,theta_fin)))\n",
        "#print(\"Accuracy (correct classification) is \"+str(test_classifier(W_fin,theta_fin)))\n",
        "train_using_API"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}